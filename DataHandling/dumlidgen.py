# -*- coding: utf-8 -*-
"""Untitle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18ZZ9q6zW2kQYRaJ4qpbHeHLYyV8xxsWJ
"""

from __future__ import division
import torch
import torchvision
import torchvision.transforms as transforms

import torch.nn as nn
import torch.nn.functional as F

import os
import numpy as np
import pandas as pd
import glob
import random

import sklearn.metrics

from torch.autograd import Variable
from torch import optim, nn

look_back = 40
input_dim = 80
hidden_dim = 128

def lstm_data(f):
    df = pd.read_csv(f,usecols=list(range(0,80)),encoding='utf-16')
    #print(df.head())
    df = df.iloc[:127,:]
    data = df.astype(np.float32)
    X = np.array(data) 
    N,D=X.shape
    print(X.shape)
    Xdata=[] 
    Ydata=[]
    #print(df.head())
    mu = X.mean(axis=0)
    std = X.std(axis=0)
    np.place(std, std == 0, 1) #so that the standard deviation never becomes zero.
    X = (X - mu) / std # normalize the data
    f1 = os.path.splitext(f)[0]  # gives first name of file without extension
    
    fpath = f1[48:]
    
    print("fpath:  ",fpath)
    clas = f1[44:47]
    print(clas)
    if (clas == 'hin'):
        Y = 3
    elif (clas == 'pun'):
        Y = 9
    Y=Y*np.ones(N)
    Y=np.array([Y])

    Y=np.array([Y])
    for i in range(0,len(X)-look_back,look_back):            
        a=X[i:(i+look_back),:]
        #print("#######"+str(i)+"####################")
        #print(len(a))        
        Xdata.append(a) 
    #print(len(X)-(len(X)%look_back))
    #print(X[len(X)-(len(X)%look_back):,:])
    Xdata1 = []
    Xdata1.append(X[len(X)-(len(X)%look_back):,:])  
    Xdata1=np.array(Xdata1) 
    Xdata1 = torch.from_numpy(Xdata1).float()    
    #print(type(Xdata))
    Xdata=np.array(Xdata)
    #print(Xdata[3].shape)
    Xdata = torch.from_numpy(Xdata).float()
    Y = torch.from_numpy(Y).long()
    print('The shape of data after appending look_back:', Xdata.shape)
    return Xdata, Xdata1,Y,fpath

class LSTMNet(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTMNet, self).__init__()
        self.hidden_dim = hidden_dim
        self.lstm1 = nn.LSTM(input_dim, hidden_dim,bidirectional=True)
        self.lstm2 = nn.LSTM(2*hidden_dim, 64,bidirectional=True)
        self.fc1 = nn.Linear(2*64, 32)
        self.fc2 = nn.Linear(32, output_dim, bias=False)

    def forward(self, x,fpath):
        #print(x.size())
        batch_size = x.size()[1]
        h0 = Variable(torch.zeros([1, batch_size, self.hidden_dim]), requires_grad=False) #h_0 of shape (num_layers * num_directions, batch, hidden_size)
        c0 = Variable(torch.zeros([1, batch_size, self.hidden_dim]), requires_grad=False)
        fx, _ = self.lstm1.forward(x) #input of shape (seq_len, batch, input_size): h_0 and c0 of shape (num_layers * num_directions, batch, hidden_size)
        # fx= output of shape (seq_len, batch, num_directions * hidden_size):
        fx, _ = self.lstm2(fx)
        print("fx_size:",fx.size())
        fx=fx[-1]
        
        print("LID_seq_sens size:  ",fx.size())
        
        LSS=fx.detach().numpy()
        LSS_txt = LSS.astype(np.float32)
        df = pd.DataFrame(LSS_txt)
        #print(df.shape)
#        print(fx.size())
        fx= torch.tanh(self.fc1(fx))
        fx= self.fc2(fx)
#        print(fx.size())
        return df

def train(model, loss, optimizer, x_val, y_val):
    x = Variable(x_val, requires_grad=False)
    y = Variable(y_val, requires_grad=False)
    # Step 1. Remember that Pytorch accumulates gradients.
    # We need to clear them out before each instance
    # Reset gradient
    optimizer.zero_grad()

    # Forward
    fx = model.forward(x)
    output = loss.forward(fx, y)

    # Backward
    output.backward()

    # Update parameters
    optimizer.step()

    return output.item()



def predict(model, x_val,fpath):
    x = Variable(x_val, requires_grad=False)
    output = model.forward(x,fpath)
    #output=output.data.numpy().mean(axis=0)
    return output

#from google.colab import drive
#import os
#drive.mount('/content/drive')
#os.chdir('/content/drive/My Drive')
#!ls

#path =  "/content/drive/My Drive/BLSTM1_2.pth"  #model path to be used
path = "/home/administrator/Muralikrishna_H/LID/IIITH/py_models/BLSTM1_20.pth"
model=torch.load(path) 

#model = FFAttention()
model.eval()
print(model)

"""#folders1 = glob.glob("/home/administrator/SLD_19/garsh/trainbnf/hin")

files_list1 = []
for folder in folders1:
    for f in glob.glob(folder+'/*.csv'):
        files_list1.append(f)

print(len(files_list1))

folders2 = glob.glob("/home/administrator/SLD_19/garsh/trainbnf/pun")
files_list2 = []
for folder in folders2:
    for f in glob.glob(folder+'/*.csv'):
        files_list2.append(f)

print(len(files_list2))"""

#try: 
 # os.makedirs('/home/administrator/SLD_19/Garima/trainLIDsclustering/hinLID_nonoverlap/')
#except(FileExistsError):
 # print('Directory already exist')
#os.chdir('/content/drive/My Drive/')
#!ls

#try: 
 # os.makedirs('/home/administrator/SLD_19/Garima/trainLIDsclustering/telLID_nonoverlap/')
#except(FileExistsError):
 # print('Directory already exist')

#print files__list

"""T=len(files_list2)
print('Total Training files: ',T)
random.shuffle(files_list2)            
Tru=[]
Pred=[]"""

"""i=0
for fn in files_list2:
    if i<3000:
        print('Reading file: ',fn)"""
#df = pd.read_csv('/home/administrator/SLD_19/Garima/train_BNF/hin/hin9.csv', encoding='utf-16') 
#print(df.head()) 
X, X1, Y ,fpath= lstm_data('/home/administrator/SLD_19/Garima/train_BNF/hin/hin9.csv')   
#df = pd.read_csv('/home/administrator/SLD_19/Garima/train_BNF/hin/hin9.csv') 
#print(df.head())       
X = np.swapaxes(X, 0, 1)  
df = predict(model,X,fpath)

X1 = np.swapaxes(X1, 0, 1)  
df1 = predict(model,X1,fpath)

fx = pd.concat([df, df1])
print(fx.shape)




















