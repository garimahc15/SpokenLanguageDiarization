# -*- coding: utf-8 -*-
"""Untitle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18ZZ9q6zW2kQYRaJ4qpbHeHLYyV8xxsWJ
"""
# fror training
from __future__ import division
import torch
import torchvision
import torchvision.transforms as transforms

import torch.nn as nn
import torch.nn.functional as F

import os
import numpy as np
import pandas as pd
import glob
import random

import sklearn.metrics

from torch.autograd import Variable
from torch import optim, nn

look_back = 40
input_dim = 80
hidden_dim = 128

def lstm_data(f):
    df = pd.read_csv(f,header=None)
    data = df.astype(np.float32)
    X = np.array(data) 
    N,D=X.shape
    #print(X.shape)
    Xdata=[] 
    Ydata=[]
    
    mu = X.mean(axis=0)
    std = X.std(axis=0)
    np.place(std, std == 0, 1) #so that the standard deviation never becomes zero.
    X = (X - mu) / std # normalize the data
    f1 = os.path.splitext(f)[0]  # gives first name of file without extension
    
    fpath = f1[46:]
    
    print("fpath:  ",fpath)
    clas = f1[42:45]
    print(clas)
    if (clas == 'hin'):
        Y = 3
    elif (clas == 'pun'):
        Y = 9
    Y=Y*np.ones(N)
    Y=np.array([Y])

    Y=np.array([Y])
    for i in range(0,len(X)-look_back,look_back):            
        a=X[i:(i+look_back),:]
        #print(len(a))        
        Xdata.append(a)        
    #print(len(Xdata))
    Xdata=np.array(Xdata)
    Xdata = torch.from_numpy(Xdata).float()
    Y=torch.from_numpy(Y).long()
    #print('The shape of data after appending look_back:', Xdata.shape)
    return Xdata,Y,fpath

class LSTMNet(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTMNet, self).__init__()
        self.hidden_dim = hidden_dim
        self.lstm1 = nn.LSTM(input_dim, hidden_dim,bidirectional=True)
        self.lstm2 = nn.LSTM(2*hidden_dim, 64,bidirectional=True)
        self.fc1 = nn.Linear(2*64, 32)
        self.fc2 = nn.Linear(32, output_dim, bias=False)

    def forward(self, x,fpath):
        batch_size = x.size()[1]
        h0 = Variable(torch.zeros([1, batch_size, self.hidden_dim]), requires_grad=False) #h_0 of shape (num_layers * num_directions, batch, hidden_size)
        c0 = Variable(torch.zeros([1, batch_size, self.hidden_dim]), requires_grad=False)
        fx, _ = self.lstm1.forward(x) #input of shape (seq_len, batch, input_size): h_0 and c0 of shape (num_layers * num_directions, batch, hidden_size)
        # fx= output of shape (seq_len, batch, num_directions * hidden_size):
        fx, _ = self.lstm2(fx)
        print("fx_size:",fx.size())
        fx=fx[-1]
        
        print("LID_seq_sens size:  ",fx.size())
        
        LSS=fx.detach().numpy()
        LSS_txt = LSS.astype(np.float32)
        df = pd.DataFrame(LSS_txt)
        if fpath[:3] == 'hin':
                df.to_csv('/home/administrator/SLD_19/garsh/trainLID/hinLID/'+fpath+'.csv')
        else:
                df.to_csv('/home/administrator/SLD_19/garsh/trainLID/punLID/'+fpath+'.csv')
#        np.savetxt('/home/administrator/Muralikrishna_H/LID/IIITH/LID_seq_senones/test_LSS/'+fpath+'.txt', LSS_txt, fmt='%.18e', delimiter=' ')
#        print(fx.size())
        fx= F.tanh(self.fc1(fx))
        fx= self.fc2(fx)
#        print(fx.size())
        return (fx)

def train(model, loss, optimizer, x_val, y_val):
    x = Variable(x_val, requires_grad=False)
    y = Variable(y_val, requires_grad=False)
    # Step 1. Remember that Pytorch accumulates gradients.
    # We need to clear them out before each instance
    # Reset gradient
    optimizer.zero_grad()

    # Forward
    fx = model.forward(x)
    output = loss.forward(fx, y)

    # Backward
    output.backward()

    # Update parameters
    optimizer.step()

    return output.item()



def predict(model, x_val,fpath):
    x = Variable(x_val, requires_grad=False)
    output = model.forward(x,fpath)
    output=output.data.numpy().mean(axis=0)
    return np.argmax(output)

#from google.colab import drive
#import os
#drive.mount('/content/drive')
#os.chdir('/content/drive/My Drive')
#!ls

#path =  "/content/drive/My Drive/BLSTM1_2.pth"  #model path to be used
path = "/home/administrator/Muralikrishna_H/LID/IIITH/py_models/BLSTM1_20.pth"
model=torch.load(path) 

#model = FFAttention()
model.eval()
print(model)

folders1 = glob.glob("/home/administrator/SLD_19/garsh/trainbnf/hin")

files_list1 = []
for folder in folders1:
    for f in glob.glob(folder+'/*.csv'):
        files_list1.append(f)

print(len(files_list1))

folders2 = glob.glob("/home/administrator/SLD_19/garsh/trainbnf/pun")
files_list2 = []
for folder in folders2:
    for f in glob.glob(folder+'/*.csv'):
        files_list2.append(f)

print(len(files_list2))

#try: 
 # os.makedirs('/home/administrator/SLD_19/Garima/trainLIDsclustering/hinLID_nonoverlap/')
#except(FileExistsError):
 # print('Directory already exist')
#os.chdir('/content/drive/My Drive/')
#!ls

#try: 
 # os.makedirs('/home/administrator/SLD_19/Garima/trainLIDsclustering/telLID_nonoverlap/')
#except(FileExistsError):
 # print('Directory already exist')

#print files__list

T=len(files_list2)
print('Total Training files: ',T)
random.shuffle(files_list2)            
Tru=[]
Pred=[]

i=0
for fn in files_list2:
    if i<3000:
        print('Reading file: ',fn)
        X, Y ,fpath= lstm_data(fn)             
        X = np.swapaxes(X, 0, 1)  
        P = predict(model,X,fpath)
        i=i+1
    else:
        break

T=len(files_list1)
print('Total Training files: ',T)
random.shuffle(files_list1)            
Tru=[]
Pred=[]

i=0
for fn in files_list1:
    if i<3000:
        print('Reading file: ',fn)
        X, Y ,fpath= lstm_data(fn)  
        #print(X.shape)           
        X = np.swapaxes(X, 0, 1)  
        P = predict(model,X,fpath)
        i = i+1
    else:
        break
























